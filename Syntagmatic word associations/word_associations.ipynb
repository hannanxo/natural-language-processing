{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c63f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')  #If you can not import stopwords, you can download using this command\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df = pd.read_csv('dataset.csv', sep='\\t', engine = 'python', encoding='WINDOWS-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4710bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering the data-set\n",
    "df = df.replace('[-\",?!.\\n()]', '', regex=True)\n",
    "df = df.replace('Ã©', 'e', regex=True)\n",
    "df['Review'] = df['Review'].str.lower()\n",
    "df['Review'] = df['Review'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04b24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stopwords from the data-set so that we only get useful associations among words\n",
    "df['Review'] = df['Review'].apply(lambda words: ' '.join([word for word in words.split() if word not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9afb8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[0:100] #if last cell is taking time slice the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fda3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = df.size\n",
    "words_dict = {}\n",
    "\n",
    "for i in range(total_reviews):\n",
    "    review = df.iloc[i]['Review']\n",
    "    tokenList = word_tokenize(''.join(review)) #tokenizes the words in the review and adds their count as well    \n",
    "    \n",
    "    for token in tokenList:\n",
    "        if token in words_dict.keys():#if word is present get it's old count and add 1\n",
    "            words_dict.update({token : words_dict[token] + 1})\n",
    "        \n",
    "        else: #if word is not present in the dict add it and its value=1\n",
    "            words_dict.update({token : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6077cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   print(words_dict) #now we've got the count of every word\n",
    "len(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7783f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {}\n",
    "for key, value in words_dict.items():\n",
    "     probs.update({key : value / total_reviews}) \n",
    "    #divide each value of the word with the total number of words in the doc to get the prob of each word\n",
    "\n",
    "# print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64aa6bc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "nested_dict = {}\n",
    "\n",
    "for i in range(total_reviews): #take each review\n",
    "    index = 0\n",
    "    nextWord = 1\n",
    "    word = 0\n",
    "    review = df.iloc[i]['Review']\n",
    "    tokenList = word_tokenize(''.join(review)) #tokenize the words\n",
    "    \n",
    "    for token in tokenList: #for each token\n",
    "        if(word < len(tokenList)):\n",
    "            probA = tokenList[word]\n",
    "            \n",
    "            for i in range(len(tokenList)):#check token with every other token\n",
    "                if(nextWord + 1 < len(tokenList)):\n",
    "                    probB = tokenList[nextWord]\n",
    "                    if probA in nested_dict.keys():#if it is already present update it's count\n",
    "                        if probB in nested_dict[token].keys():#if the key in nested dictionary is already present update its val\n",
    "                            nested_dict[probA][probB] = nested_dict[probA][probB] + 1\n",
    "                        else:#else add it with value of 1\n",
    "                            nested_dict[probA][probB] = 1\n",
    "                    else:#for first time add it with value of 1\n",
    "                          nested_dict[probA] = {probB : 1}\n",
    "\n",
    "                    nextWord = nextWord + 1\n",
    "                    index = index + 1\n",
    "            word = word + 1\n",
    "            nextWord = word + 1\n",
    "            index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "330bf010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nested_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f437ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame({}, columns = ['A', 'B', 'MI Score'])\n",
    "\n",
    "#iterate throught the nested dictionary and find the entropy and conditional entropy\n",
    "#use then the mutual information score using the difference of two\n",
    "for word in nested_dict.keys(): \n",
    "    for nextWord in nested_dict[word]:\n",
    "        probA = probs[word]\n",
    "        countA = words_dict[word]\n",
    "        countB = words_dict[nextWord]\n",
    "        countAwithBINT = nested_dict[word][nextWord]\n",
    "        countAwithB = float(countAwithBINT)\n",
    "        probAwithB = countAwithB / countB\n",
    "        \n",
    "        if(probA > probAwithB):\n",
    "            try:\n",
    "                entropyA = - 1 * probA * (math.log(probA, 2) + 0.1)\n",
    "                entropyAtoB = -1 * probAwithB * (math.log(probAwithB, 2))  - (1 - probAwithB) * (math.log(1 - probAwithB, 2))\n",
    "                mi = -(entropyA - entropyAtoB) \n",
    "#                 print(mi)\n",
    "                scores = scores.append({'A': word, 'B':nextWord, 'MI Score': mi}, ignore_index = True)\n",
    "            except ValueError:\n",
    "                continue\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7289abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               A         B  MI Score\n",
      "0          place       use  0.709799\n",
      "1          place       may  0.709799\n",
      "2          place     right  0.709799\n",
      "3          place  congrats  0.709799\n",
      "4          place      work  0.709799\n",
      "...          ...       ...       ...\n",
      "1487       areas     hotel -0.200555\n",
      "1488       seixo     hotel -0.212116\n",
      "1489  everything     place -0.219841\n",
      "1490        food       one -0.250750\n",
      "1491   fantastic       one -0.257682\n",
      "\n",
      "[1492 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#will take quite some time to print. You can slice the reviews to test\n",
    "scores = scores.sort_values(by = 'MI Score', ascending = False)\n",
    "scores = scores.reset_index(drop = True)\n",
    "scores = scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf1336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
